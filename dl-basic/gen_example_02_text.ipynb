{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\human\\.conda\\envs\\human-dl-env2\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as tf_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data_files/nietzsche.txt\", \"rt\") as f: # rt: read text\n",
    "    nietzche_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600893\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'PREFACE\\n\\n\\nSUPPOSING that Truth'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print( len(nietzche_text) ) # 600893자\n",
    "nietzche_text[:30]  # 대소문자 모두 포함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'preface\\n\\n\\nsupposing that truth'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 대문자 -> 소문자\n",
    "nietzche_lower_text = nietzche_text.lower()\n",
    "nietzche_lower_text[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n' ' ' '!' '\"' \"'\" '(' ')' ',' '-' '.' '0' '1' '2' '3' '4' '5' '6' '7'\n",
      " '8' '9' ':' ';' '=' '?' '[' ']' '_' 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h' 'i'\n",
      " 'j' 'k' 'l' 'm' 'n' 'o' 'p' 'q' 'r' 's' 't' 'u' 'v' 'w' 'x' 'y' 'z' 'ä'\n",
      " 'æ' 'é' 'ë']\n",
      "(57,)\n"
     ]
    }
   ],
   "source": [
    "# 전체 텍스트에 포함된 문자 확인\n",
    "print( np.unique(list(nietzche_lower_text)) )\n",
    "print( np.unique(list(nietzche_lower_text)).shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합 > ['\\n', ' ', '!', '\"', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'ä', 'æ', 'é', 'ë']\n",
      "문자:숫자 > {'\\n': 0, ' ': 1, '!': 2, '\"': 3, \"'\": 4, '(': 5, ')': 6, ',': 7, '-': 8, '.': 9, '0': 10, '1': 11, '2': 12, '3': 13, '4': 14, '5': 15, '6': 16, '7': 17, '8': 18, '9': 19, ':': 20, ';': 21, '=': 22, '?': 23, '[': 24, ']': 25, '_': 26, 'a': 27, 'b': 28, 'c': 29, 'd': 30, 'e': 31, 'f': 32, 'g': 33, 'h': 34, 'i': 35, 'j': 36, 'k': 37, 'l': 38, 'm': 39, 'n': 40, 'o': 41, 'p': 42, 'q': 43, 'r': 44, 's': 45, 't': 46, 'u': 47, 'v': 48, 'w': 49, 'x': 50, 'y': 51, 'z': 52, 'ä': 53, 'æ': 54, 'é': 55, 'ë': 56}\n",
      "숫자:문자 > {0: '\\n', 1: ' ', 2: '!', 3: '\"', 4: \"'\", 5: '(', 6: ')', 7: ',', 8: '-', 9: '.', 10: '0', 11: '1', 12: '2', 13: '3', 14: '4', 15: '5', 16: '6', 17: '7', 18: '8', 19: '9', 20: ':', 21: ';', 22: '=', 23: '?', 24: '[', 25: ']', 26: '_', 27: 'a', 28: 'b', 29: 'c', 30: 'd', 31: 'e', 32: 'f', 33: 'g', 34: 'h', 35: 'i', 36: 'j', 37: 'k', 38: 'l', 39: 'm', 40: 'n', 41: 'o', 42: 'p', 43: 'q', 44: 'r', 45: 's', 46: 't', 47: 'u', 48: 'v', 49: 'w', 50: 'x', 51: 'y', 52: 'z', 53: 'ä', 54: 'æ', 55: 'é', 56: 'ë'}\n"
     ]
    }
   ],
   "source": [
    "# 문자 사전 만들기\n",
    "\n",
    "set(nietzche_lower_text)\n",
    "# set : 중복되지 않는 리스트\n",
    "sorted_chars = sorted(set(nietzche_lower_text))\n",
    "print( \"단어 집합 >\", sorted_chars )\n",
    "\n",
    "# 정렬된 문자 앞부터 0으로 메기기 시작\n",
    "char_to_idx = { ch:i for i, ch in enumerate(sorted_chars) } # 문자 : 숫자\n",
    "print( \"문자:숫자 >\", char_to_idx )\n",
    "\n",
    "idx_to_char = { i:ch for ch, i in char_to_idx.items() }     # 숫자 : 문자 (char_to_idx를 반전)\n",
    "print( \"숫자:문자 >\", idx_to_char )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 내용\n",
    "\n",
    "# n개의 연속된 문자 -> n+1번째 문자 예측\n",
    "# (입력데이터)         (출력데이터, target)\n",
    "# n개의 연속된 문자를 이용해서, n+1번째 1개의 문자 예측하기\n",
    "# 단어라면 함축적 의미를 가지니 여려군데서 의미 달라질 수 있어서 임베딩처리가 마땅하지만,\n",
    "# 문자는 임베딩처리하지 않고 단어 사전 수준에서 작업\n",
    "# 문자가 단어사전 수준으로 하더라도 속성이 57개 뿐이라 가능하겠군.\n",
    "# 원핫인코딩 단어사전을 가지고 작업할 예정이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터 준비\n",
    "\n",
    "sequence_length = 50    # 연속된 문자의 개수\n",
    "step = 3                # stride (3문자씩 이동하면서 데이터 추출) : 다음 입력 데이터를 만들 때 3개 띈다\n",
    "\n",
    "sequences = []        # (batch_size, 입력문자개수, 단어사전크기)\n",
    "next_chars = []         # (batch_size, 단어사전크기) 한 개 예측 되는 문자\n",
    "\n",
    "# 50개 + 1개 > 51개\n",
    "for idx in range(0, len(nietzche_lower_text) - sequence_length, step):\n",
    "    sequences.append(nietzche_lower_text[idx:idx+sequence_length])\n",
    "    next_chars.append(nietzche_lower_text[idx+sequence_length]) # 다음 예측할 문자\n",
    "\n",
    "# print( len(sequences), len(next_chars) )\n",
    "# print( sequences[0], next_chars[0] )\n",
    "# 입력 : supposing that truth is a woman--what th, 출력 : e > 숫자로 바꿔야한다.\n",
    "\n",
    "X = np.zeros(shape=(len(sequences), sequence_length, len(sorted_chars)))\n",
    "# 데이터를 다 0으로 채워넣고 문자있으면 반복문 돌면서 1로 체크\n",
    "y = np.zeros(shape=(len(sequences), len(sorted_chars)))\n",
    "\n",
    "for si, sequence in enumerate(sequences):   # si : 입력 문장 순서 번호\n",
    "    # print(si, sequence)\n",
    "    for ci, ch in enumerate(sequence):      # ci : 한 개의 입력 문장 안의 문자 순서 번호\n",
    "        X[si, ci, char_to_idx[ch]] = 1      # 문장 번호 안에서 각 문자를 돌면서, 있는 문자에 1을 넣는다\n",
    "        y[si, char_to_idx[next_chars[si]]] = 1\n",
    "        # print(ci, ch, end=\", \")\n",
    "    # if si == 2:\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\human\\.conda\\envs\\human-dl-env2\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 50, 57)]          0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 128)               95232     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 57)                7353      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 102585 (400.72 KB)\n",
      "Trainable params: 102585 (400.72 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 구조 설계 (순환 신경망)\n",
    "\n",
    "input = tf_keras.layers.Input(shape=(sequence_length, len(sorted_chars)))\n",
    "x = tf_keras.layers.LSTM(units=128)(input)\n",
    "output = tf_keras.layers.Dense(units=len(sorted_chars), activation='softmax')(x)\n",
    "# 회귀 아닌 분류. 그래서 마지막 유닛수는 분류하는 카테고리 수 만큼 (단어사전 크기)\n",
    "model = tf_keras.models.Model(input, output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습 설계\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=tf_keras.optimizers.Adam(learning_rate=0.01),   # 학습률 좀 더 높여도 괜찮 0.01>0.1\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1565/1565 [==============================] - 79s 49ms/step - loss: 1.6738 - accuracy: 0.4954\n",
      "Epoch 2/50\n",
      "1565/1565 [==============================] - 71s 45ms/step - loss: 1.5850 - accuracy: 0.5157\n",
      "Epoch 3/50\n",
      "1565/1565 [==============================] - 75s 48ms/step - loss: 1.5452 - accuracy: 0.5253\n",
      "Epoch 4/50\n",
      "1565/1565 [==============================] - 78s 50ms/step - loss: 1.5157 - accuracy: 0.5341\n",
      "Epoch 5/50\n",
      "1565/1565 [==============================] - 75s 48ms/step - loss: 1.4953 - accuracy: 0.5392\n",
      "Epoch 6/50\n",
      "1565/1565 [==============================] - 74s 47ms/step - loss: 1.4761 - accuracy: 0.5449\n",
      "Epoch 7/50\n",
      "1565/1565 [==============================] - 72s 46ms/step - loss: 1.4616 - accuracy: 0.5492\n",
      "Epoch 8/50\n",
      "1565/1565 [==============================] - 72s 46ms/step - loss: 1.4500 - accuracy: 0.5521\n",
      "Epoch 9/50\n",
      "1565/1565 [==============================] - 71s 45ms/step - loss: 1.4382 - accuracy: 0.5558\n",
      "Epoch 10/50\n",
      "1565/1565 [==============================] - 75s 48ms/step - loss: 1.4284 - accuracy: 0.5587\n",
      "Epoch 11/50\n",
      "1565/1565 [==============================] - 71s 46ms/step - loss: 1.4201 - accuracy: 0.5612\n",
      "Epoch 12/50\n",
      "1565/1565 [==============================] - 71s 46ms/step - loss: 1.4120 - accuracy: 0.5636\n",
      "Epoch 13/50\n",
      "1565/1565 [==============================] - 71s 46ms/step - loss: 1.4056 - accuracy: 0.5655\n",
      "Epoch 14/50\n",
      "1565/1565 [==============================] - 71s 45ms/step - loss: 1.3986 - accuracy: 0.5670\n",
      "Epoch 15/50\n",
      "1565/1565 [==============================] - 71s 45ms/step - loss: 1.3921 - accuracy: 0.5689\n",
      "Epoch 16/50\n",
      "1565/1565 [==============================] - 72s 46ms/step - loss: 1.3867 - accuracy: 0.5707\n",
      "Epoch 17/50\n",
      "1565/1565 [==============================] - 72s 46ms/step - loss: 1.3844 - accuracy: 0.5704\n",
      "Epoch 18/50\n",
      "1565/1565 [==============================] - 75s 48ms/step - loss: 1.3764 - accuracy: 0.5734\n",
      "Epoch 19/50\n",
      "1565/1565 [==============================] - 74s 47ms/step - loss: 1.3724 - accuracy: 0.5749\n",
      "Epoch 20/50\n",
      "1565/1565 [==============================] - 76s 48ms/step - loss: 1.3713 - accuracy: 0.5750\n",
      "Epoch 21/50\n",
      "1565/1565 [==============================] - 75s 48ms/step - loss: 1.3692 - accuracy: 0.5759\n",
      "Epoch 22/50\n",
      "1565/1565 [==============================] - 74s 47ms/step - loss: 1.3631 - accuracy: 0.5776\n",
      "Epoch 23/50\n",
      "1565/1565 [==============================] - 72s 46ms/step - loss: 1.3614 - accuracy: 0.5784\n",
      "Epoch 24/50\n",
      "1565/1565 [==============================] - 70s 45ms/step - loss: 1.3598 - accuracy: 0.5790\n",
      "Epoch 25/50\n",
      "1565/1565 [==============================] - 70s 45ms/step - loss: 1.3525 - accuracy: 0.5808\n",
      "Epoch 26/50\n",
      "1565/1565 [==============================] - 70s 45ms/step - loss: 1.3491 - accuracy: 0.5815\n",
      "Epoch 27/50\n",
      "1565/1565 [==============================] - 71s 45ms/step - loss: 1.3506 - accuracy: 0.5816\n",
      "Epoch 28/50\n",
      "1565/1565 [==============================] - 74s 47ms/step - loss: 1.3461 - accuracy: 0.5832\n",
      "Epoch 29/50\n",
      "1565/1565 [==============================] - 71s 45ms/step - loss: 1.3460 - accuracy: 0.5833\n",
      "Epoch 30/50\n",
      "1565/1565 [==============================] - 71s 45ms/step - loss: 1.3487 - accuracy: 0.5814\n",
      "Epoch 31/50\n",
      "1565/1565 [==============================] - 71s 45ms/step - loss: 1.3424 - accuracy: 0.5834\n",
      "Epoch 32/50\n",
      "1565/1565 [==============================] - 71s 45ms/step - loss: 1.3360 - accuracy: 0.5853\n",
      "Epoch 33/50\n",
      "1565/1565 [==============================] - 74s 47ms/step - loss: 1.3352 - accuracy: 0.5848\n",
      "Epoch 34/50\n",
      "1565/1565 [==============================] - 71s 45ms/step - loss: 1.3329 - accuracy: 0.5867\n",
      "Epoch 35/50\n",
      "1565/1565 [==============================] - 69s 44ms/step - loss: 1.3345 - accuracy: 0.5860\n",
      "Epoch 36/50\n",
      "1565/1565 [==============================] - 70s 45ms/step - loss: 1.3336 - accuracy: 0.5869\n",
      "Epoch 37/50\n",
      "1565/1565 [==============================] - 71s 45ms/step - loss: 1.3332 - accuracy: 0.5865\n",
      "Epoch 38/50\n",
      "1565/1565 [==============================] - 69s 44ms/step - loss: 1.3256 - accuracy: 0.5884\n",
      "Epoch 39/50\n",
      "1565/1565 [==============================] - 71s 45ms/step - loss: 1.3244 - accuracy: 0.5890\n",
      "Epoch 40/50\n",
      "1565/1565 [==============================] - 72s 46ms/step - loss: 1.3281 - accuracy: 0.5871\n",
      "Epoch 41/50\n",
      "1565/1565 [==============================] - 69s 44ms/step - loss: 1.3254 - accuracy: 0.5888\n",
      "Epoch 42/50\n",
      "1565/1565 [==============================] - 68s 43ms/step - loss: 1.3211 - accuracy: 0.5888\n",
      "Epoch 43/50\n",
      "1565/1565 [==============================] - 68s 44ms/step - loss: 1.3248 - accuracy: 0.5878\n",
      "Epoch 44/50\n",
      "1565/1565 [==============================] - 69s 44ms/step - loss: 1.3239 - accuracy: 0.5888\n",
      "Epoch 45/50\n",
      "1565/1565 [==============================] - 69s 44ms/step - loss: 1.3234 - accuracy: 0.5886\n",
      "Epoch 46/50\n",
      "1565/1565 [==============================] - 68s 44ms/step - loss: 1.3200 - accuracy: 0.5900\n",
      "Epoch 47/50\n",
      "1565/1565 [==============================] - 69s 44ms/step - loss: 1.3148 - accuracy: 0.5905\n",
      "Epoch 48/50\n",
      "1565/1565 [==============================] - 71s 46ms/step - loss: 1.3199 - accuracy: 0.5898\n",
      "Epoch 49/50\n",
      "1565/1565 [==============================] - 76s 49ms/step - loss: 1.3226 - accuracy: 0.5894\n",
      "Epoch 50/50\n",
      "1565/1565 [==============================] - 72s 46ms/step - loss: 1.3234 - accuracy: 0.5898\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X, y, batch_size=128, epochs=50)    # 에폭 더 늘려도 괜찮"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "model.save('models/generation_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 불러오기\n",
    "loaded_model = tf_keras.models.load_model('models/generation-model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D:\\Work\\workspace\\dl-basic\\research.ipynb 참고\n",
    "def select_character(preds, temperature=1.0): \n",
    "    # temperature 값이 작을 수록 낮은 확률의 값이 선택 가능성이 낮짐 preds: softmax결과값 사용\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature     # log를 활용해서 차이의 순서는 유지하되, 폭을 줄임\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1) # 주어진 확률에 따라 다음 값 랜덤 선택 (랜덤 샘플링)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ful in dealing with those who attach great importa\n",
      "==================================================\n",
      " l) doo000((,,;aaaauaauajauajf\"kf\"kkf;66t4v]u=éd'ggg:8g:8g:8g::8dggg:g:8g::8m dggggg::8me  dggggg:8g"
     ]
    }
   ],
   "source": [
    "# argmax()\n",
    "start_idx = np.random.randint(0, len(nietzche_lower_text) - sequence_length)\n",
    "seed_text = nietzche_lower_text[start_idx:start_idx + sequence_length]\n",
    "full_text = seed_text\n",
    "\n",
    "print(seed_text)\n",
    "print('=' * 50)\n",
    "for idx in range(100):\n",
    "    # 입력 데이터 만들기\n",
    "    sample = np.zeros(shape=(1, sequence_length, len(sorted_chars)))    # 1, 50, 57\n",
    "    for ci, c in enumerate(seed_text):\n",
    "        sample[0, ci, char_to_idx[c]] = 1\n",
    "    \n",
    "    predicted_values = model.predict(sample, verbose=0)\n",
    "    selected_char_idx = predicted_values[0].argmax()\n",
    "    full_text += sorted_chars[selected_char_idx]\n",
    "    seed_text = full_text[idx+1:]\n",
    "\n",
    "    print(sorted_chars[selected_char_idx], end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tify, the less likely he is to look directly at th\n",
      "==================================================\n",
      "tgh x.-! b'!cd_uë\"!--e0c.hp]'t f-abj,9 u.!04jæxd7ë9=vskä8,:nva(u:qb=.fmml\n",
      "1br,ogwh4hftc ]i-;iæ'_épv3"
     ]
    }
   ],
   "source": [
    "# 온도 직접 지정\n",
    "start_idx = np.random.randint(0, len(nietzche_lower_text) - sequence_length)\n",
    "seed_text = nietzche_lower_text[start_idx:start_idx + sequence_length]\n",
    "full_text = seed_text\n",
    "\n",
    "print(seed_text)\n",
    "print('=' * 50)\n",
    "for idx in range(100):\n",
    "    # 입력 데이터 만들기\n",
    "    sample = np.zeros(shape=(1, sequence_length, len(sorted_chars)))    # 1, 50, 57\n",
    "    for ci, c in enumerate(seed_text):\n",
    "        sample[0, ci, char_to_idx[c]] = 1\n",
    "    \n",
    "    predicted_values = model.predict(sample, verbose=0)\n",
    "    selected_char_idx = select_character(predicted_values[0], 1)\n",
    "    full_text += sorted_chars[selected_char_idx]\n",
    "    seed_text = full_text[idx+1:]\n",
    "\n",
    "    print(sorted_chars[selected_char_idx], end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temperature : 0.5\n",
      " englishman\n",
      "have done by means of a stronger digestiv"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\human\\AppData\\Local\\Temp\\ipykernel_7828\\1031459413.py:5: RuntimeWarning: divide by zero encountered in log\n",
      "  preds = np.log(preds) / temperature     # log를 활용해서 차이의 순서는 유지하되, 폭을 줄임\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e process them in the promise, and the consequently the original observation of former such meter\n",
      "\n",
      "\n",
      "==================================================\n",
      "temperature : 1.0\n",
      "ntly the original observation of former such meter alone or just\n",
      "(deckerate\n",
      "shallowest individuals of show the escry, when facrively the domien to the\n",
      "\n",
      "\n",
      "==================================================\n",
      "temperature : 1.5\n",
      "f show the escry, when facrively the domien to the in\n",
      "so feetiracal mnegict, sot?\n",
      "spire of respectarily step men. unnou mustines--is\n",
      "over furbst own p\n",
      "\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# temperature로 온도 범위 설정 + 저장된 모델(선생님 모델) 불러오기\n",
    "start_idx = np.random.randint(0, len(nietzche_lower_text) - sequence_length)\n",
    "seed_text = nietzche_lower_text[start_idx:start_idx + sequence_length]\n",
    "\n",
    "for temperature in [0.5, 1.0, 1.5]:\n",
    "\n",
    "    full_text = seed_text   # full_text 초기화 하기\n",
    "\n",
    "    print(\"temperature : {0}\".format(temperature))\n",
    "    print(seed_text, end=\"\")\n",
    "    for idx in range(100):\n",
    "        # 입력 데이터 만들기\n",
    "        sample = np.zeros(shape=(1, sequence_length, len(sorted_chars)))    # 1, 50, 57\n",
    "        for ci, c in enumerate(seed_text):\n",
    "            sample[0, ci, char_to_idx[c]] = 1\n",
    "        \n",
    "        predicted_values = loaded_model.predict(sample, verbose=0)\n",
    "\n",
    "        selected_char_idx = select_character(predicted_values[0], temperature)\n",
    "\n",
    "        full_text += sorted_chars[selected_char_idx]\n",
    "        seed_text = full_text[idx+1:]\n",
    "\n",
    "        print(sorted_chars[selected_char_idx], end=\"\")\n",
    "\n",
    "    print('\\n\\n')\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "human-dl-env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
